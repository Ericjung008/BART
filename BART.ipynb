{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries that will be used.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import stats\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import geopy.distance\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a SparkSession.**\n",
    "* _**SparkSession is an entry point to program in Spark.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('BART').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the 2016 and 2017 BART ridership data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016 = spark.read.csv('date-hour-soo-dest-2016.csv', inferSchema = True, header = True)\n",
    "df2017 = spark.read.csv('date-hour-soo-dest-2017.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge the two dataframes into one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+-------------------+\n",
      "|Origin|Destination|Throughput|           DateTime|\n",
      "+------+-----------+----------+-------------------+\n",
      "|  12TH|       12TH|         1|2016-01-01 00:00:00|\n",
      "|  12TH|       16TH|         1|2016-01-01 00:00:00|\n",
      "|  12TH|       24TH|         4|2016-01-01 00:00:00|\n",
      "|  12TH|       ASHB|         4|2016-01-01 00:00:00|\n",
      "|  12TH|       BALB|         2|2016-01-01 00:00:00|\n",
      "+------+-----------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df = df2016.union(df2017)\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm the dataframes have been merged correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2016.count() + df2017.count() == merged_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the station data and read first five rows to ensure the dataset has been loaded correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Abbreviation | 12TH                                                                                                                                                                                                                                                                                     \n",
      " Description  | 1245 Broadway, Oakland CA 94612<br />12th St. Oakland City Center Station is in the heart of Downtown Oakland, near historic Old Oakland and Oakland's Chinatown.                                                                                                                        \n",
      " Location     | -122.271450,37.803768,0                                                                                                                                                                                                                                                                  \n",
      " Name         | 12th St. Oakland City Center (12TH)                                                                                                                                                                                                                                                      \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Abbreviation | 16TH                                                                                                                                                                                                                                                                                     \n",
      " Description  | 2000 Mission Street, San Francisco CA 94110<br />\"The Mission\" refers to the San Francisco de Asis Mission, also known as Mission Dolores, which was founded 1776. Today the neighborhood is host to an eclectic mix of restaurants, markets, performance spaces, shops, and nightspots. \n",
      " Location     | -122.419694,37.765062,0                                                                                                                                                                                                                                                                  \n",
      " Name         | 16th St. Mission (16TH)                                                                                                                                                                                                                                                                  \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Abbreviation | 19TH                                                                                                                                                                                                                                                                                     \n",
      " Description  | 1900 Broadway, Oakland CA 94612<br />19th Street Station is in the heart of Uptown Oakland near the historic Paramount Theater on Broadway.                                                                                                                                              \n",
      " Location     | -122.268602,37.808350,0                                                                                                                                                                                                                                                                  \n",
      " Name         | 19th St. Oakland (19TH)                                                                                                                                                                                                                                                                  \n",
      "-RECORD 3------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Abbreviation | 24TH                                                                                                                                                                                                                                                                                     \n",
      " Description  | 2800 Mission Street, San Francisco CA 94110<br />\"The Mission\" refers to the San Francisco de Asis Mission, also known as Mission Dolores, which was founded 1776. Today the neighborhood is host to an eclectic mix of restaurants, markets, performance spaces, shops, and nightspots. \n",
      " Location     | -122.418143,37.752470,0                                                                                                                                                                                                                                                                  \n",
      " Name         | 24th St. Mission (24TH)                                                                                                                                                                                                                                                                  \n",
      "-RECORD 4------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Abbreviation | ASHB                                                                                                                                                                                                                                                                                     \n",
      " Description  | 3100 Adeline Street, Berkeley CA 94703<br />Ashby Station is located on Ashby Avenue and Adeline Street in the southern part of Berkeley. On weekends, the City of Berkeley sponsors a popular flea market in the parking lot.                                                           \n",
      " Location     | -122.270062,37.852803,0                                                                                                                                                                                                                                                                  \n",
      " Name         | Ashby (ASHB)                                                                                                                                                                                                                                                                             \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station = spark.read.csv('station_info.csv', header = True, escape = '\\\"')\n",
    "station.show(5, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the address from the \"Description\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Address(text):\n",
    "    address = text.split('<br')[0]\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|Address                                    |\n",
      "+-------------------------------------------+\n",
      "|1245 Broadway, Oakland CA 94612            |\n",
      "|2000 Mission Street, San Francisco CA 94110|\n",
      "|1900 Broadway, Oakland CA 94612            |\n",
      "|2800 Mission Street, San Francisco CA 94110|\n",
      "|3100 Adeline Street, Berkeley CA 94703     |\n",
      "+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf = F.udf(get_Address)\n",
    "station = station.withColumn('Address', udf(station.Description))\n",
    "station.select('Address').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using regular expressions, remove the abbreviated text from the \"Name\" column for each row.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(text):\n",
    "    clean_text = re.sub('\\(\\w*\\)', '', text)\n",
    "    final_text = clean_text.rstrip()\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|Name                        |\n",
      "+----------------------------+\n",
      "|12th St. Oakland City Center|\n",
      "|16th St. Mission            |\n",
      "|19th St. Oakland            |\n",
      "|24th St. Mission            |\n",
      "|Ashby                       |\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf = F.udf(remove_text)\n",
    "cleaned_df = station.withColumn('Name', udf(station.Name))\n",
    "cleaned_df.select(F.col('Name')).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create latitude and longitude columns by extracting the values from the \"Location\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat(location):\n",
    "    split_list = location.split(',')\n",
    "    latitude = split_list[1]\n",
    "    return latitude\n",
    "\n",
    "def get_lng(location):\n",
    "    split_list = location.split(',')\n",
    "    longitude = split_list[0]\n",
    "    return longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "| Latitude|  Longitude|\n",
      "+---------+-----------+\n",
      "| 37.80377| -122.27145|\n",
      "| 37.76506| -122.41969|\n",
      "| 37.80835|  -122.2686|\n",
      "| 37.75247|-122.418144|\n",
      "|37.852802|-122.270065|\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_lat = F.udf(get_lat)\n",
    "split_df = cleaned_df.withColumn('Latitude', udf_lat(cleaned_df.Location).cast('float').alias('Latitude'))\n",
    "\n",
    "udf_lng = F.udf(get_lng)\n",
    "split_df = split_df.withColumn('Longitude', udf_lng(cleaned_df.Location).cast('float').alias('Longitude'))\n",
    "\n",
    "split_df.select('Latitude', 'Longitude').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve the important columns and read the first five rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------+-------------------------------------------+---------+-----------+\n",
      "|Abbreviation|Name                        |Address                                    |Latitude |Longitude  |\n",
      "+------------+----------------------------+-------------------------------------------+---------+-----------+\n",
      "|12TH        |12th St. Oakland City Center|1245 Broadway, Oakland CA 94612            |37.80377 |-122.27145 |\n",
      "|16TH        |16th St. Mission            |2000 Mission Street, San Francisco CA 94110|37.76506 |-122.41969 |\n",
      "|19TH        |19th St. Oakland            |1900 Broadway, Oakland CA 94612            |37.80835 |-122.2686  |\n",
      "|24TH        |24th St. Mission            |2800 Mission Street, San Francisco CA 94110|37.75247 |-122.418144|\n",
      "|ASHB        |Ashby                       |3100 Adeline Street, Berkeley CA 94703     |37.852802|-122.270065|\n",
      "+------------+----------------------------+-------------------------------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp_df = split_df.select(['Abbreviation', 'Name', 'Address', 'Latitude', 'Longitude'])\n",
    "imp_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next objective is to join the 2016 and 2017 merged dataframe with the station dataframe using the column with the abbreviated station names from each dataframe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before the two dataframes are joined, the unique values from each column needs to be checked for inconsistencies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lists(list1,list2):\n",
    "    print('Checking if names from first list is in second list..')\n",
    "    for name in list1:\n",
    "        if name not in list2:\n",
    "            print('\"{}\" is not a common value.'.format(name))\n",
    "            print('\\n')\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    print('Checking if names from second list is in first list..')\n",
    "    for name in list2:\n",
    "        if name not in list1:\n",
    "            print('\"{}\" is not a common value.'.format(name))\n",
    "            print('\\n')\n",
    "        else: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = imp_df.select('Abbreviation').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "origin_names = merged_df.select('Origin').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "dest_names = merged_df.select('Destination').distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if names from first list is in second list..\n",
      "\"WARM\" is not a common value.\n",
      "\n",
      "\n",
      "Checking if names from second list is in first list..\n",
      "\"WSPR\" is not a common value.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_lists(station_names,origin_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if names from first list is in second list..\n",
      "\"WARM\" is not a common value.\n",
      "\n",
      "\n",
      "Checking if names from second list is in first list..\n",
      "\"WSPR\" is not a common value.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_lists(station_names,dest_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if names from first list is in second list..\n",
      "Checking if names from second list is in first list..\n",
      "All names are common in both columns.\n"
     ]
    }
   ],
   "source": [
    "if compare_lists(origin_names,dest_names) == None:\n",
    "    print('All names are common in both columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The abbreviated station name \"WARM\" is missing in the \"Origin\" and \"Destination\" columns whereas \"WSPR\" is missing in the \"Abbreviation\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "|Abbreviation|Name                      |Address                                   |Latitude|Longitude  |\n",
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "|WARM        |Warm Springs/South Fremont|45193 Warm Springs Blvd, Fremont, CA 94539|37.50217|-121.939316|\n",
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp_df.where(F.col('Abbreviation') == 'WARM').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+-------------------+\n",
      "|Origin|Destination|Throughput|           DateTime|\n",
      "+------+-----------+----------+-------------------+\n",
      "|  WSPR|       WSPR|         1|2016-08-11 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-08-17 11:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-08-18 10:00:00|\n",
      "|  WSPR|       WSPR|         2|2016-08-24 10:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-08-26 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-06 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-12 15:00:00|\n",
      "|  WSPR|       WSPR|         4|2016-09-13 09:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-19 14:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-21 11:00:00|\n",
      "|  WSPR|       WSPR|         4|2016-09-21 12:00:00|\n",
      "|  WSPR|       WSPR|        12|2016-09-22 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-27 18:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-09-28 13:00:00|\n",
      "|  WSPR|       WSPR|        13|2016-09-29 11:00:00|\n",
      "|  WSPR|       WSPR|         5|2016-09-29 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-10-15 16:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-10-17 14:00:00|\n",
      "|  WSPR|       WSPR|        11|2016-10-22 12:00:00|\n",
      "|  WSPR|       WSPR|         1|2016-11-06 19:00:00|\n",
      "+------+-----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.where((F.col('Origin') == 'WSPR') & (F.col('Destination') == 'WSPR')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the rows, it can be inferred \"WSPR\" is an abbreviation for Warm Springs. \"WARM\" and \"WSPR\" should actually have the same abbreviated name.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change WARM to WSPR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_df(df,colmn,word_to_replace,new_word):\n",
    "    dataframe = df.withColumn(colmn, F.when(F.col(colmn) == word_to_replace, new_word).otherwise(F.col(colmn)))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "|Abbreviation|Name                      |Address                                   |Latitude|Longitude  |\n",
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "|WSPR        |Warm Springs/South Fremont|45193 Warm Springs Blvd, Fremont, CA 94539|37.50217|-121.939316|\n",
      "+------------+--------------------------+------------------------------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp_df = changed_df(imp_df,'Abbreviation','WARM', 'WSPR')\n",
    "imp_df.where(F.col('Abbreviation') == 'WSPR').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the merged and station dataframe, creating columns to give more information about the origin station.**  \n",
    "**Change the column names to better represent the columns.**  \n",
    "**Drop the \"Abbreviation\" column since the column is a duplicate.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = [merged_df.Origin == imp_df.Abbreviation]\n",
    "join_df1 = merged_df.join(imp_df, on = cond1, how = 'left')\n",
    "join_df1 = join_df1.withColumnRenamed('Name', 'Origin Name')\n",
    "join_df1 = join_df1.withColumnRenamed('Address', 'Origin Address')\n",
    "join_df1 = join_df1.withColumnRenamed('Latitude', 'Ori_lat')\n",
    "join_df1 = join_df1.withColumnRenamed('Longitude', 'Ori_lng')\n",
    "join_df1 = join_df1.drop(F.col('Abbreviation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the same approach to add more columns for the destination station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = [merged_df.Destination == imp_df.Abbreviation]\n",
    "join_df2 = join_df1.join(imp_df, on = cond2, how = 'left')\n",
    "join_df2 = join_df2.withColumnRenamed('Name', 'Destination Name')\n",
    "join_df2 = join_df2.withColumnRenamed('Address', 'Destination Address')\n",
    "join_df2 = join_df2.withColumnRenamed('Latitude', 'Dest_lat')\n",
    "join_df2 = join_df2.withColumnRenamed('Longitude', 'Dest_lng')\n",
    "join_df2 = join_df2.drop(F.col('Abbreviation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the column names into a list in the preferred order the new dataframe will be viewed.**  \n",
    "**The ordered dataframe will be saved into the variable \"spark_df.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------\n",
      " DateTime            | 2016-01-01 00:00:00                         \n",
      " Origin              | 12TH                                        \n",
      " Origin Name         | 12th St. Oakland City Center                \n",
      " Origin Address      | 1245 Broadway, Oakland CA 94612             \n",
      " Ori_lat             | 37.80377                                    \n",
      " Ori_lng             | -122.27145                                  \n",
      " Destination         | 12TH                                        \n",
      " Destination Name    | 12th St. Oakland City Center                \n",
      " Destination Address | 1245 Broadway, Oakland CA 94612             \n",
      " Dest_lat            | 37.80377                                    \n",
      " Dest_lng            | -122.27145                                  \n",
      " Throughput          | 1                                           \n",
      "-RECORD 1----------------------------------------------------------\n",
      " DateTime            | 2016-01-01 00:00:00                         \n",
      " Origin              | 12TH                                        \n",
      " Origin Name         | 12th St. Oakland City Center                \n",
      " Origin Address      | 1245 Broadway, Oakland CA 94612             \n",
      " Ori_lat             | 37.80377                                    \n",
      " Ori_lng             | -122.27145                                  \n",
      " Destination         | 16TH                                        \n",
      " Destination Name    | 16th St. Mission                            \n",
      " Destination Address | 2000 Mission Street, San Francisco CA 94110 \n",
      " Dest_lat            | 37.76506                                    \n",
      " Dest_lng            | -122.41969                                  \n",
      " Throughput          | 1                                           \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_order_list = ['DateTime',\n",
    "                  'Origin',\n",
    "                  'Origin Name',\n",
    "                  'Origin Address',\n",
    "                  'Ori_lat',\n",
    "                  'Ori_lng',\n",
    "                  'Destination',\n",
    "                  'Destination Name',\n",
    "                  'Destination Address',\n",
    "                  'Dest_lat',\n",
    "                  'Dest_lng',\n",
    "                  'Throughput']\n",
    "\n",
    "spark_df = join_df2.select(col_order_list)\n",
    "spark_df.show(2, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for null values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls(df):\n",
    "    null_df = df.select([F.count(F.when(F.isnan(col), col)).alias(col) for col in df.columns])\n",
    "    return null_df.show(vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " DateTime            | 0   \n",
      " Origin              | 0   \n",
      " Origin Name         | 0   \n",
      " Origin Address      | 0   \n",
      " Ori_lat             | 0   \n",
      " Ori_lng             | 0   \n",
      " Destination         | 0   \n",
      " Destination Name    | 0   \n",
      " Destination Address | 0   \n",
      " Dest_lat            | 0   \n",
      " Dest_lng            | 0   \n",
      " Throughput          | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nulls(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Imputing null values won't be necessary for this project.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see if the data type for each column is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Origin Name: string (nullable = true)\n",
      " |-- Origin Address: string (nullable = true)\n",
      " |-- Ori_lat: float (nullable = true)\n",
      " |-- Ori_lng: float (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Destination Name: string (nullable = true)\n",
      " |-- Destination Address: string (nullable = true)\n",
      " |-- Dest_lat: float (nullable = true)\n",
      " |-- Dest_lng: float (nullable = true)\n",
      " |-- Throughput: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note: The \"DateTime\" column is incorrectly labeled as a string.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the data type of \"DateTime\" column to \"timestamp.\"  \n",
    "This step will be critical for gaining insight and for modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: timestamp (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Origin Name: string (nullable = true)\n",
      " |-- Origin Address: string (nullable = true)\n",
      " |-- Ori_lat: float (nullable = true)\n",
      " |-- Ori_lng: float (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Destination Name: string (nullable = true)\n",
      " |-- Destination Address: string (nullable = true)\n",
      " |-- Dest_lat: float (nullable = true)\n",
      " |-- Dest_lng: float (nullable = true)\n",
      " |-- Throughput: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark_df.withColumn('DateTime', F.to_timestamp(F.col('DateTime'),'yyyy-MM-dd HH:mm:ss').alias('DateTime'))\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see if there were minutes and seconds data collected.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(minute)=0)\n"
     ]
    }
   ],
   "source": [
    "dt_minute = spark_df.select(F.minute('DateTime').alias('minute'))\n",
    "max_minute = dt_minute.agg({'minute': 'max'}).collect()[0]\n",
    "print(max_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(second)=0)\n"
     ]
    }
   ],
   "source": [
    "dt_second = spark_df.select(F.second('DateTime').alias('second'))\n",
    "max_second = dt_second.agg({'second': 'max'}).collect()[0]\n",
    "print(max_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the max amount for minute and second is 0, I will only be focusing on hour for the time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create month, day, and hour columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------\n",
      " DateTime            | 2016-01-01 00:00:00                         \n",
      " Origin              | 12TH                                        \n",
      " Origin Name         | 12th St. Oakland City Center                \n",
      " Origin Address      | 1245 Broadway, Oakland CA 94612             \n",
      " Ori_lat             | 37.80377                                    \n",
      " Ori_lng             | -122.27145                                  \n",
      " Destination         | 12TH                                        \n",
      " Destination Name    | 12th St. Oakland City Center                \n",
      " Destination Address | 1245 Broadway, Oakland CA 94612             \n",
      " Dest_lat            | 37.80377                                    \n",
      " Dest_lng            | -122.27145                                  \n",
      " month               | Jan                                         \n",
      " day                 | Fri                                         \n",
      " hour                | 0                                           \n",
      " Throughput          | 1                                           \n",
      "-RECORD 1----------------------------------------------------------\n",
      " DateTime            | 2016-01-01 00:00:00                         \n",
      " Origin              | 12TH                                        \n",
      " Origin Name         | 12th St. Oakland City Center                \n",
      " Origin Address      | 1245 Broadway, Oakland CA 94612             \n",
      " Ori_lat             | 37.80377                                    \n",
      " Ori_lng             | -122.27145                                  \n",
      " Destination         | 16TH                                        \n",
      " Destination Name    | 16th St. Mission                            \n",
      " Destination Address | 2000 Mission Street, San Francisco CA 94110 \n",
      " Dest_lat            | 37.76506                                    \n",
      " Dest_lng            | -122.41969                                  \n",
      " month               | Jan                                         \n",
      " day                 | Fri                                         \n",
      " hour                | 0                                           \n",
      " Throughput          | 1                                           \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month = F.date_format('DateTime','MMM').alias('month')\n",
    "day = F.date_format('DateTime', 'E').alias('day')\n",
    "hour = F.hour('DateTime').alias('hour')\n",
    "\n",
    "spark_df = spark_df.select('DateTime',\n",
    "                           'Origin',\n",
    "                           'Origin Name',\n",
    "                           'Origin Address',\n",
    "                           'Ori_lat',\n",
    "                           'Ori_lng',\n",
    "                           'Destination',\n",
    "                           'Destination Name',\n",
    "                           'Destination Address',\n",
    "                           'Dest_lat',\n",
    "                           'Dest_lng',\n",
    "                            month,\n",
    "                            day,\n",
    "                            hour,\n",
    "                           'Throughput')\n",
    "\n",
    "spark_df.show(2, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step of the project is to analyze the data to answer important questions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Which BART station is the busiest?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. _**Group spark the dataframe by the unique values in the \"Origin\" column and assign the grouped dataframe into a variable. The full station name and the total number of ridership for each origin station will be retained. Change the column names to preferred names.**_\n",
    "2. _**Use the same approach to create a separate dataframe for \"Destination.\"**_\n",
    "3. _**Merge the two dataframes vertically.**_\n",
    "4. _**Group the merged dataframe by the unique values in the \"Abbreviation\" column. Similar to the previous step, the full station name and the total number of ridership for each station will be retained.**_  \n",
    "5. _**Sort the newly created dataframe in descending order and output the first row to find out which station is the busiest.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------+-----------------+\n",
      "|Abbreviation|Name                        |Num_of_passengers|\n",
      "+------------+----------------------------+-----------------+\n",
      "|NCON        |North Concord/Martinez      |1101650          |\n",
      "|POWL        |Powell St.                  |14138322         |\n",
      "|CIVC        |Civic Center/UN Plaza       |10145110         |\n",
      "|12TH        |12th St. Oakland City Center|5322811          |\n",
      "|16TH        |16th St. Mission            |5365000          |\n",
      "+------------+----------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origin = spark_df.groupBy('Origin').agg({'Origin Name': 'first', 'Throughput': 'sum'})\n",
    "origin = origin.toDF(*['Abbreviation','Name','Num_of_passengers'])\n",
    "\n",
    "dest = spark_df.groupBy('Destination').agg({'Destination Name': 'first', 'Throughput': 'sum'})\n",
    "dest = dest.toDF(*['Abbreviation','Name','Num_of_passengers'])\n",
    "\n",
    "con_df = origin.union(dest)\n",
    "con_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+\n",
      "|Abbreviation|       Name|Tot_passengers|\n",
      "+------------+-----------+--------------+\n",
      "|        EMBR|Embarcadero|      34076644|\n",
      "+------------+-----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "con_df = con_df.groupBy('Abbreviation').agg({'Name': 'first', 'Num_of_passengers': 'sum'})\n",
    "con_df = con_df.toDF(*['Abbreviation','Name','Tot_passengers'])\n",
    "con_df.sort(F.col('Tot_passengers').desc()).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: Busiest station is Embarcadero.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: What is the least popular BART route?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. _**Group the spark dataframe by each unique origin and destination combination. The full origin and destination station names will be retained. The total number of ridership will be calculated for each unique combination.**_\n",
    "2. _**Change the column names to preferred names and change the order columns are viewed in the dataframe.**_\n",
    "3. _**Sort the grouped dataframe in ascending order and output the first row to find out which route is the least popular.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------+-----------+----------------+--------------+\n",
      "|Origin|Origin Name               |Destination|Destination Name|Tot_passengers|\n",
      "+------+--------------------------+-----------+----------------+--------------+\n",
      "|WSPR  |Warm Springs/South Fremont|SBRN       |San Bruno       |40            |\n",
      "+------+--------------------------+-----------+----------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = spark_df.groupBy(['Origin','Destination']).agg({'Origin Name': 'first','Destination Name': 'first','Throughput': 'sum'})\n",
    "grouped_df = grouped_df.toDF(*['Origin', 'Destination', 'Origin Name', 'Destination Name', 'Tot_passengers'])\n",
    "reordered_df = grouped_df.select(['Origin', 'Origin Name', 'Destination', 'Destination Name', 'Tot_passengers'])\n",
    "least_pop = reordered_df.orderBy('Tot_passengers', ascending = True).show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: The least popular route is from Warm Springs/South Fremont to San Bruno.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: When is the best time to go to SF from Berkeley if you want to find a seat?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. _**Pull the distinct origin addresses located in Berkeley.**_\n",
    "2. _**Pull the distinct destination addresses located in San Francisco.**_\n",
    "3. _**Save the abbreviated origin names in a list and the abbreviated destination names in another list.**_\n",
    "4. _**Retrieve only the rows from the spark dataframe if the \"Origin\" column involves Berkeley and if the \"Destination\" column involves \"San Francisco.\"**_\n",
    "5. _**Group the filtered dataframe by the unique combination of origin name, destination name, and hour of the ridership. Add the total number of ridership for each of these combination and change the column names to the desired names.**_\n",
    "6. _**Group the grouped dataframe just by the unique combinations of origin and destinations and retrieve the minimum number of passenges for each combination.**_\n",
    "7. _**Join the first grouped dataframe and the dataframe containing the minimum values. This will give insight to the optimal time to travel from Berkeley to San Francisco for each combination.**_\n",
    "\n",
    "**Note: Steps 1 and 2 should give an idea of how many different combinations there are.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------------------------------+\n",
      "|Origin|Origin Name      |Origin Address                           |\n",
      "+------+-----------------+-----------------------------------------+\n",
      "|ASHB  |Ashby            |3100 Adeline Street, Berkeley CA 94703   |\n",
      "|DBRK  |Downtown Berkeley|2160 Shattuck Avenue, Berkeley CA 94704  |\n",
      "|NBRK  |North Berkeley   |1750 Sacramento Street, Berkeley CA 94702|\n",
      "+------+-----------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Berk = spark_df.select('Origin', 'Origin Name', 'Origin Address').where(F.col('Origin Address').contains('Berkeley'))\n",
    "Berk.distinct().show(Berk.distinct().count(), truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------+---------------------------------------------------------------------+\n",
      "|Destination|Destination Name           |Destination Address                                                  |\n",
      "+-----------+---------------------------+---------------------------------------------------------------------+\n",
      "|16TH       |16th St. Mission           |2000 Mission Street, San Francisco CA 94110                          |\n",
      "|GLEN       |Glen Park                  |2901 Diamond Street, San Francisco CA 94131                          |\n",
      "|MONT       |Montgomery St.             |598 Market Street, San Francisco CA 94104                            |\n",
      "|24TH       |24th St. Mission           |2800 Mission Street, San Francisco CA 94110                          |\n",
      "|SFIA       |San Francisco Int'l Airport|International Terminal, Level 3, San Francisco Int'l Airport CA 94128|\n",
      "|CIVC       |Civic Center/UN Plaza      |1150 Market Street, San Francisco CA 94102                           |\n",
      "|BALB       |Balboa Park                |401 Geneva Avenue, San Francisco CA 94112                            |\n",
      "|EMBR       |Embarcadero                |298 Market Street, San Francisco CA 94111                            |\n",
      "|POWL       |Powell St.                 |899 Market Street, San Francisco CA 94102                            |\n",
      "|SSAN       |South San Francisco        |1333 Mission Road, South San Francisco CA 94080                      |\n",
      "+-----------+---------------------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SF = spark_df.select('Destination', 'Destination Name', 'Destination Address').where(F.col('Destination Address').contains('San Francisco'))\n",
    "SF.distinct().show(SF.distinct().count(), truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "berk_list = [row['Origin'] for row in Berk.distinct().collect()]\n",
    "SF_list = [row['Destination'] for row in SF.distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+----------+\n",
      "|Origin|Destination|hour|Passengers|\n",
      "+------+-----------+----+----------+\n",
      "|  NBRK|       CIVC|   5|      1406|\n",
      "|  ASHB|       GLEN|  14|       596|\n",
      "|  DBRK|       MONT|  20|      7887|\n",
      "|  DBRK|       CIVC|  22|      8316|\n",
      "|  NBRK|       24TH|  22|      1178|\n",
      "+------+-----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = spark_df.filter((spark_df['Origin'].isin(berk_list)) & (spark_df['Destination'].isin(SF_list)))\n",
    "grouped_df = filtered_df.groupBy(['Origin','Destination','hour']).agg({'Throughput': 'sum'})\n",
    "grouped_df = grouped_df.withColumnRenamed('sum(Throughput)', 'Passengers')\n",
    "grouped_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+\n",
      "|Origin|Destination|Passengers|\n",
      "+------+-----------+----------+\n",
      "|  DBRK|       POWL|         3|\n",
      "|  ASHB|       24TH|         5|\n",
      "|  NBRK|       GLEN|         1|\n",
      "|  ASHB|       EMBR|         2|\n",
      "|  DBRK|       CIVC|         8|\n",
      "+------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df = grouped_df.groupBy(['Origin','Destination']).agg({'Passengers': 'min'})\n",
    "min_df = min_df.withColumnRenamed('min(Passengers)','Passengers')\n",
    "min_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+----+\n",
      "|Origin|Destination|Passengers|hour|\n",
      "+------+-----------+----------+----+\n",
      "|  ASHB|       GLEN|         1|   4|\n",
      "|  ASHB|       EMBR|         2|   3|\n",
      "|  ASHB|       24TH|         5|   3|\n",
      "|  ASHB|       16TH|         1|   3|\n",
      "|  ASHB|       BALB|        51|   1|\n",
      "|  ASHB|       POWL|         1|   2|\n",
      "|  ASHB|       SSAN|        14|   1|\n",
      "|  ASHB|       CIVC|         4|   2|\n",
      "|  ASHB|       SFIA|         1|   4|\n",
      "|  ASHB|       SFIA|         1|   2|\n",
      "|  ASHB|       MONT|         3|   3|\n",
      "|  DBRK|       CIVC|         8|   2|\n",
      "|  DBRK|       POWL|         3|   3|\n",
      "|  DBRK|       BALB|         3|   4|\n",
      "|  DBRK|       SFIA|         1|   2|\n",
      "|  DBRK|       MONT|         9|   2|\n",
      "|  DBRK|       EMBR|         2|   3|\n",
      "|  DBRK|       GLEN|         2|   2|\n",
      "|  DBRK|       SFIA|         1|   3|\n",
      "|  DBRK|       24TH|         4|   3|\n",
      "|  DBRK|       SSAN|         2|   3|\n",
      "|  DBRK|       16TH|         1|   3|\n",
      "|  NBRK|       16TH|         2|   3|\n",
      "|  NBRK|       24TH|         2|   3|\n",
      "|  NBRK|       EMBR|         3|   2|\n",
      "|  NBRK|       BALB|         1|   2|\n",
      "|  NBRK|       POWL|         4|   2|\n",
      "|  NBRK|       MONT|         2|   2|\n",
      "|  NBRK|       CIVC|         4|   2|\n",
      "|  NBRK|       SSAN|         7|   1|\n",
      "|  NBRK|       MONT|         2|   3|\n",
      "|  NBRK|       SFIA|         1|   3|\n",
      "|  NBRK|       GLEN|         1|   2|\n",
      "+------+-----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = min_df.join(grouped_df, on = ['Origin','Destination','Passengers'], how = 'left')\n",
    "best.orderBy('Origin').show(best.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: The dataframe above represents the best time to travel to each station in San Francisco City from various stations in Berkeley.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: Which day of the week is the busiest?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. _**Group the spark dataframe by the day and sum up the ridership that fall under the same day. Change the column names as desired.**_\n",
    "2. _**Order the grouped dataframe in descending order.**_\n",
    "3. _**Output the first row to find out which day is the busiest.**_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(day='Wed', Num_of_passengers=30677189)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_day = spark_df.groupBy('day').agg({'Throughput': 'sum'})\n",
    "grouped_day = grouped_day.withColumnRenamed('sum(Throughput)','Num_of_passengers')\n",
    "busy_day = grouped_day.orderBy(F.col('Num_of_passengers').desc())\n",
    "busy_day.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: Busiest day is Wednesday.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5: How many people take the BART late at night?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "1. _**Retrieve the rows that fall between the start of a late night and the end of a late night.**_\n",
    "2. _**Add the number of ridership from the retrived rows.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count how many people use BART in a given time period\n",
    "def people_count(df):\n",
    "    try:\n",
    "        start_hr = input('Please enter the starting hour: ')\n",
    "        end_hr = input('Please enter the ending hour: ')\n",
    "        \n",
    "        int_start = int(start_hr)\n",
    "        int_end = int(end_hr)\n",
    "    \n",
    "    except:\n",
    "        print('Please enter an integer number.')\n",
    "    \n",
    "    filtered_df = df.where((F.col('hour') >= int_start) | (F.col('hour') <= int_end))\n",
    "    result = filtered_df.agg({'Throughput': 'sum'}).collect()[0]\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Calculating...')\n",
    "    \n",
    "    return result['sum(Throughput)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the starting hour: 23\n",
      "Please enter the ending hour: 5\n",
      "\n",
      "\n",
      "Calculating...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6939024"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_count(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: Assuming late night is from 11:00PM to 5:00AM, the number of people who use BART during this timeframe is 6,939,024.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the straight line distance between every station.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate distance in miles using the coordinates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(ori_lat, ori_lng, dest_lat, dest_lng):\n",
    "    ori_coord = (ori_lat, ori_lng)\n",
    "    dest_coord = (dest_lat, dest_lng)\n",
    "    \n",
    "    return round(geopy.distance.distance(ori_coord, dest_coord).miles, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = F.udf(get_dist, DoubleType())\n",
    "spark_df = spark_df.withColumn('Distance', udf('Ori_lat','Ori_lng','Dest_lat','Dest_lng'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " Origin              | HAYW                                            \n",
      " Origin Address      | 699 'B' Street, Hayward CA 94541                \n",
      " Destination         | COLM                                            \n",
      " Destination Address | 365 D Street, Colma CA 94014                    \n",
      " Distance            | 20.81                                           \n",
      "-RECORD 1--------------------------------------------------------------\n",
      " Origin              | PITT                                            \n",
      " Origin Address      | 1700 West Leland Road, Pittsburg CA 94565       \n",
      " Destination         | BALB                                            \n",
      " Destination Address | 401 Geneva Avenue, San Francisco CA 94112       \n",
      " Distance            | 34.28                                           \n",
      "-RECORD 2--------------------------------------------------------------\n",
      " Origin              | SSAN                                            \n",
      " Origin Address      | 1333 Mission Road, South San Francisco CA 94080 \n",
      " Destination         | BALB                                            \n",
      " Destination Address | 401 Geneva Avenue, San Francisco CA 94112       \n",
      " Distance            | 3.96                                            \n",
      "-RECORD 3--------------------------------------------------------------\n",
      " Origin              | 24TH                                            \n",
      " Origin Address      | 2800 Mission Street, San Francisco CA 94110     \n",
      " Destination         | COLM                                            \n",
      " Destination Address | 365 D Street, Colma CA 94014                    \n",
      " Distance            | 5.37                                            \n",
      "-RECORD 4--------------------------------------------------------------\n",
      " Origin              | SSAN                                            \n",
      " Origin Address      | 1333 Mission Road, South San Francisco CA 94080 \n",
      " Destination         | SHAY                                            \n",
      " Destination Address | 28601 Dixon Street, Hayward CA 94544            \n",
      " Distance            | 21.31                                           \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist_df = spark_df.groupBy(['Origin','Origin Address','Destination','Destination Address']).agg({'Distance':'first'})\n",
    "dist_df = dist_df.withColumnRenamed('first(Distance)','Distance')\n",
    "dist_df.show(5, truncate = False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step of the project is to build a model that can predict the number of people commuting to work by Bart between any 2 stations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before data can be used to build a model, numerical data must be checked for normality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(df, colmn):\n",
    "    \n",
    "    before_mean = df.select(F.mean(F.col(colmn)).alias('before_mean')).collect()[0]['before_mean']\n",
    "    before_median = df.approxQuantile(colmn,[0.5], 0.25)[0]\n",
    "    before_skew = df.select(F.skewness(F.col(colmn)).alias('before_skew')).collect()[0]['before_skew']\n",
    "    before_kurtosis = df.select(F.kurtosis(F.col(colmn)).alias('before_kurtosis')).collect()[0]['before_kurtosis']\n",
    "    \n",
    "    tf_df = df.select(F.log(F.col(colmn) + 1).alias(colmn))\n",
    "    \n",
    "    after_mean = tf_df.select(F.mean(F.col(colmn)).alias('after_mean')).collect()[0]['after_mean']\n",
    "    after_median = tf_df.approxQuantile(colmn,[0.5], 0.25)[0]\n",
    "    after_skew = tf_df.select(F.skewness(F.col(colmn)).alias('after_skew')).collect()[0]['after_skew']\n",
    "    after_kurtosis = tf_df.select(F.kurtosis(F.col(colmn)).alias('after_kurtosis')).collect()[0]['after_kurtosis']\n",
    "    \n",
    "    stats_df = spark.createDataFrame(\n",
    "        [\n",
    "            ('before', before_mean, before_median, before_skew, before_kurtosis),\n",
    "            ('after', after_mean, after_median, after_skew, after_kurtosis)\n",
    "        ],\n",
    "        ['index','mean', 'median', 'skew', 'kurtosis']\n",
    "    )\n",
    "    return stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+-------------------+-------------------+\n",
      "| index|              mean|            median|               skew|           kurtosis|\n",
      "+------+------------------+------------------+-------------------+-------------------+\n",
      "|before|13.632914338481893|              13.0|-0.2864554716759491|-0.6325473419843961|\n",
      "| after|2.5615035320970487|2.6390573296152584| -2.381418878422813|  7.430904673149209|\n",
      "+------+------------------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformation(spark_df, 'hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+-------------------+--------------------+\n",
      "| index|              mean|           median|               skew|            kurtosis|\n",
      "+------+------------------+-----------------+-------------------+--------------------+\n",
      "|before|13.560962073081413|            11.79|0.45672145109303297|-0.48874095132369444|\n",
      "| after|  2.46183935686871|2.548663615590751|-1.1543890523218658|  1.3531670255355017|\n",
      "+------+------------------+-----------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformation(spark_df, 'Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Since the skewness of the hour and distance column is close to 0, a feature transformation will not be necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, the hour and distance columns must be normalized.**  \n",
    "**This is so one of the columns won't have a bigger impact on the prediction than the other column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['hour','Distance']\n",
    "\n",
    "assemblers = [VectorAssembler(inputCols = [col], outputCol = col + '_vec') for col in cols_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol = col + '_vec', outputCol = col + '_scaled') for col in cols_to_scale]\n",
    "pipeline = Pipeline(stages = assemblers + scalers)\n",
    "scalerModel = pipeline.fit(spark_df)\n",
    "scaledDF = scalerModel.transform(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, dummies are created for the catergorical features so they can be used for modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_list = scaledDF.select('Origin').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "origin_dum = [F.when(F.col('Origin') == ori,1).otherwise(0).alias('Ori_'+ ori) for ori in ori_list]\n",
    "\n",
    "dest_list = scaledDF.select('Destination').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "dest_dum = [F.when(F.col('Destination') == dest, 1).otherwise(0).alias('Dest_' + dest) for dest in dest_list]\n",
    "\n",
    "month_list = scaledDF.select('month').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "month_dum = [F.when(F.col('month') == m, 1).otherwise(0).alias('month_' + m) for m in month_list]\n",
    "\n",
    "day_list = scaledDF.select('day').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "day_dum = [F.when(F.col('day') == d, 1).otherwise(0).alias('day_' + d) for d in day_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BART = scaledDF.select(*origin_dum,*dest_dum,*month_dum,*day_dum,'hour_scaled','Distance_scaled','Throughput')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Apache Spark, the independent features must be converted into one vector column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_col_names = BART.columns[0:-1]\n",
    "featureAssembler = VectorAssembler(inputCols = ind_col_names, outputCol = 'Independent Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|Independent Features|Throughput|\n",
      "+--------------------+----------+\n",
      "|(113,[3,49,101,10...|         1|\n",
      "|(113,[3,50,101,10...|         1|\n",
      "|(113,[3,91,101,10...|         4|\n",
      "|(113,[3,66,101,10...|         4|\n",
      "|(113,[3,90,101,10...|         2|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF = featureAssembler.transform(BART)\n",
    "finalDF = finalDF.select('Independent Features','Throughput')\n",
    "finalDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data is split into training and testing dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = finalDF.randomSplit([0.7,0.3])\n",
    "trainingDF = splits[0]\n",
    "testingDF = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three algorithms are compared and the model with the least error between the predicted values and the actual values is chosen to be the best model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(features, label, train, test):\n",
    "    r2_list = ['r2']\n",
    "    rmse_list = ['RMSE']\n",
    "    \n",
    "    lr = LinearRegression(featuresCol = features, labelCol = label)\n",
    "    rf = RandomForestRegressor(featuresCol = features, labelCol = label)\n",
    "    gb = GBTRegressor(featuresCol = features, labelCol = label)\n",
    "    \n",
    "    list_of_algorithms = [lr, rf, gb]\n",
    "    \n",
    "    for ind in range(len(list_of_algorithms)):\n",
    "        model = list_of_algorithms[ind].fit(train)\n",
    "        pred = model.transform(test)\n",
    "        \n",
    "        rmse_evaluator = RegressionEvaluator(labelCol = label, predictionCol = 'prediction', metricName = 'rmse')\n",
    "        rmse = round(rmse_evaluator.evaluate(pred, params = {}), 3)\n",
    "        rmse_list.append(rmse)\n",
    "        \n",
    "        r2_evaluator = RegressionEvaluator(labelCol = label, predictionCol = 'prediction', metricName = 'r2')\n",
    "        r2 = round(r2_evaluator.evaluate(pred, params = {}), 3)\n",
    "        r2_list.append(r2)\n",
    "        \n",
    "    metrics_df = spark.createDataFrame(\n",
    "        [\n",
    "            (r2_list),\n",
    "            (rmse_list)\n",
    "        ],\n",
    "        ['Metric','Linear Regression','Random Forest','Gradient Boosting']\n",
    "    )\n",
    "    \n",
    "    return metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------------+-----------------+\n",
      "|Metric|Linear Regression|Random Forest|Gradient Boosting|\n",
      "+------+-----------------+-------------+-----------------+\n",
      "|    r2|            0.166|        0.257|            0.578|\n",
      "|  RMSE|           30.551|       28.825|           21.721|\n",
      "+------+-----------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_evaluation('Independent Features', 'Throughput', trainingDF, testingDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
